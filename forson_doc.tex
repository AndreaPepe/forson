\documentclass[a4paper,12pt]{article}

\title{Forson: a sentence generation tool}
\author{A. Tarantini}



\begin{document}
\maketitle
\begin{center}
Copyright (c)  2005  Alfonso Tarantini

This project has been developed in the SW Lab. of  DEI - Politecnico di Milano,
under the supervision of Prof. S. Crespi Reghizzi and G.P. Agosta
\end{center}

\bigskip

\begin{abstract}
A program for syntactically correct sentence generation is described. The program takes a Bison grammar file as input, and provides random or coverage testing sentences of the language defined by the grammar. The program can be used for compiler or parser testing or didactical purposes.
\end{abstract}

\bigskip

\section{Introduction}
Forson is a program for generation of syntactically correct sentences on the basis of an input Bison grammar file.
The program does not rely on a particular structure for the input file: any generic Bison grammar file is accepted as long as it is syntactically correct (in other words, if it's ok for Bison, it will be ok for Forson). Dynamic data structures are used to avoid restricting the scope of the possible target grammars.

There are two operating modes. In ``random'' mode sentences are generated by the means of the Grow algorithm applied to an automatically created stochastic grammar. In ``coverage'' mode, the sentence generation procedes in a deterministic fashion, following the Purdom algorithm. In this case a set of sentences is generated such that all grammar rules are used. Minimality of this set often occurs, but is not guaranteed.

In both cases the sentence validity does not exceed the syntactic level. There are no built-in facilities to ensure the semantic conformance to the target language. A partial support to the lexical aspect is instead provided, as it is possible for the user to provide a lexicon input file containing a list of lexical values for the terminal symbols of the target language.

Forson is written in ISO C under Linux. It should be portable to any POSIX compliant system.

Forson is a batch program; an explanation of program invocation and options is provided, after which the document's sections follow the operations of the control flow.


\section{Invocation and options}
\noindent
The program must be invocated in this form:

\begin{verbatim}
forson [OPTION]* GRAMMAR_FILE [LEXICON_FILE]
\end{verbatim}

\noindent
The input grammar file is mandatory, while the lexicon file is optional.

\noindent
Forson also supports a number of options, in both short and long form:

\begin{description}
\item[-c, --coverage:]
Enables the ``coverage'' generation mode, the default mode is ``random''.

\item[-h, --help:]
Displays a help message which summarizes the usage information.

\item[-m, --message FILE:]
Instructs forson to print messages to FILE. Forson has a distinct message stream for printing warnings, the internal symbol table (if requested), and other informative messages during the program's execution. We can specify a FILE in which to print this information. The default is the standard output.

\item[-n, --no-spaces:]
Forson has a built-in ``blanks'' generator, which automatically intermixes spaces, tabs and newlines to the generated sentences. It's purpose is to favor readability of the output. This option instructs Forson to not generate blank text (for example if the target language is not free-form).

\item[-o, --out FILE:]
Provides the path and filename for output of the generated sentences. The default is the file ``o.out'' in the executable's directory.

\item[-O, --standard-output:]
Instructs Forson to output generated sentences to standard output instead of a file.

\item[-p, --print-tables:]
Instructs Forson to print it's internal symbol table in a readable, formatted fashion to the message stream.

\item[-r, --repeat N:]
Instructs Forson to generate N random sentences. The default is a single sentence. This value is ignored if the ``coverage'' mode is selected.

\item[-s, --separator str:]
Sets the separator between sentences to the string ``str''. The default is two newlines. If Forson is to generate only a single sentence (the default), the separator isn't used at all.

\item[-v, --verbosity N:]
Sets the verbosity level to the positive value N. Verbosity affects the amount of debugging information to be directed to the message stream. A detailed summary of the information added by different values of N:
\begin{enumerate}
\item[0.]
only warnings (default)
\item
main steps of execution described
\item
information on the activity of the input grammar file parser
\item
information on main exit cleaning operations
\item
information on the activity of the input grammar file scanner and the input lexicon file scanner
\item
tracing of calls to the sentence generation functions
\item
everything, including operations on the internal data structure
\end{enumerate}
\noindent
A value of 6 is the maximum possible; any higher value of N is defaulted to 6.

\item[-e, --version:]
Prints version information and exits successfully.
\end{description}
\noindent
Mandatory arguments to long options are mandatory for short options too.



\section{How Forson works}

Forson is a batch program. It does not provide (nor needs) an interactive user interface. It serially opens it's files, parses it's input, elaborates it's internal data structure and starts the requested sentence generation. No intermediate temporary files are involved, because the two generation algorithms (Grow and Purdom), ultimately emit the terminal symbols of the target language directly to the output stream.

All data structures are allocated dynamically, and no implicit assumptions are made about the size and characteristics of the input grammar, as long as it is syntactically correct, in terms of Bison's meta-syntax itself.

Error checking is provided at various levels, as we will see; error reporting has been taken in great account, attempting to provide as much context information as possible.



\section{The Data Structure}

All the ``symbols'' defined in the Bison input grammar file are held in a table. These include non-terminal symbols, tokens (called ``lexicals'' throughout the code), and literals (both string literals and character literals, it does not make any difference to Forson). These three types of symbols are distinguished by certain values, or ranges of values that a particular field of the table entry can hold. The details of the implementation are abstracted away by the use of boolean functions like \emph{is\_LEXICAL(S)} or \emph{is\_NT(S)} to verify the type of the generic symbol S.

The non-terminal symbols also have associated ``rules''.
For the sake of clarity, when we refer to ``the rule of a symbol S'', we mean a single rule alternative of which the non-terminal symbol S is the result. Information on rules is held in a separate table for every non-terminal symbol.

Both these types of tables, the symbol table and the rule table are actually implemented as linked lists. Anyway the operations on these structures (adding or extracting elements, etc.) are performed by an abstraction layer of functions which provide the appropriate insulation from the underlying implementation. The code for these functions is contained in the file \emph{listops.c}.



\section{First Step - Parsing The Input Grammar File}

The main function is contained in the file \emph{main.c}\ .

The first thing the program does is process the command line options. After this, Forson sets up all it's files and streams and calls the \emph{build\_tables()} function, defined in the file \emph{build\_tables.c}\ .

At this point the parsing of the input file takes place. The parser for Bison's syntax is written in Bison itself: the code is contained in the file \emph{metagrammar.y}\ . The \emph{yylex()} function, required by every Bison parser, has been provided by a Flex scanner, contained in \emph{metagrammar.lex}\ .

The parser itself constructs the internal data structure by adding symbols and rules as it recognizes them.

Not all of the Bison syntax is of interest to Forson's execution. Large parts of the input file may be ignored. For example, the C code contained in the rules (and enclosed in curly brackets) is completely ignored because it has nothing to do with the syntactic structure of the language. The same applies to many types of Bison declarations. On the other hand, Forson does not accept input files that are not sintactically correct according to Bison's meta-syntax. For example, the input file must contain at least the first ``part-separator'' (two percent signs, like this: \verb/%%/)\ and must contain at least one rule.

In Bison literals all standard C escape sequences can be present, but trigraphs are not supported. Note that the text associated with the string or character literals is copied verbatim in memory. The actual escaping of the character sequences is done only in the final generated sentences. This is to avoid messing up the formatted output of the internal data structure (option \textbf{-p}), when requested.

A special issue is caused by Bison's reserved keyword \verb/error/. It is used in Bison grammar files to define rules that will be reduced by the resulting parser in case a parsing error occures. From a sentence generation perspective, rules containing the \verb/error/ keyword should be discarded. As we will see, this can lead to particular situations, to take care of in the next phase of grammar elaboration.

During input grammar file parsing, the program can issue three kinds of errors:
\begin{itemize}
\item
lexical (E.G.: ``unrecognized character'')
\item
syntactical (E.G.: ``unexpected token: FOO, expecting: BAR'')
\item
consistence (E.G.: ``attempting to assign rule to a token'')
\end{itemize}

\noindent
In any of these cases, the error is fatal: execution is aborted.



\section{Parsing The Lexicon Input File}
After the input grammar file has been processed, and all the symbols defined, we must process the input lexicon file (if provided), in which the user gives a list of lexical values to be associated with the tokens of the target language. Every token can have more than one lexical value: the generator will just choose one at random.

The syntax of the input lexicon file has been defined ad hoc with only one objective in mind: to keep it simple. Here is the formal BNF description:

\begin{verbatim}
input_lexicon_file    ::==   {association}*
association           ::==   token_name {"lexical_value"}*;
\end{verbatim}

Where we assume \verb/token_name/ can be a generic Bison identifier and \verb/lexical_value/ can be any sequence of printing characters and spaces; newlines are not allowed between the pair of double quotes, besides this the language is free-form. Between quotes all standard C escape sequences can be present, but trigraphs are not supported. Note that the text associated with the string or character literals is copied verbatim in memory. The actual escaping of the character sequences is done only in the final generated sentences. This is to avoid messing up the formatted output of the internal data structure (option \textbf{-p} ), when requested.

\noindent
Here is an example of the contents of a valid lexicon file:

\

\noindent
\emph{Example 1:}

\begin{verbatim}
SPACES "\n" "\t\t";
NAME "John" "Paul" "Matthew";
\end{verbatim}

Thanks to the simplicity of the sintax no full-blown parser is needed. With a simple Flex scanner (file \emph{lexicon.lex}) and a while loop, the problem is effortlessly solved.
The scanner may encounter tokens that are not defined in the symbol table. This is not considered an error, the following lexical values before the next semicolon are simply ignored (with a warning).
If the scanner encounters a sintax error, the input lexicon file processing is immediately aborted, but the lexical values already associated correctly before the point in which the error occured are not discarded.



\section{Grammar Consistence Checking and Rule Normalization}

After the input grammar file parsing has been completed successfully, control is passed by \emph{main()} to function \emph{check\_grammar()}. This function performs a sequence of checks to assure that the grammar contained in the input file (which at this point is surely syntactically valid) is not affected by consistence flaws or other pathologic situations.

The first thing it does is to check if all symbols are reachable.
A symbol S is unreachable if a rule has been defined for which S is the result, but S is not a component in any rule contained by non-terminal symbols which derive from the starting symbol (or ``axiom'').
The sentence generation could successfully take place even if one or more non-terminal symbols were found to be unreachable. Instead, the program issues a fatal error. This is because an unreachable non-terminal usually means a typing mistake or by other means something different from what the user wanted. It can be difficult to track this kind of mistake by simply examining the output.
Unreachable (or unused) tokens, are instead considered less problematic, so Forson simply prints a warning, and procedes.

The next elaboration of the target grammar involves making sure that all non-terminal symbols have at least one rule. There are two different ways to have a non-terminal without any defined rules.
In the first case, the symbol is ``undefined''. This means that a symbol that appears as a result in some rule has not been declared as a token, so forson would imply it is a non-terminal. Instead it never appears as a result of any rule. This is an erratic situation, so Forson issues a fatal error.
Another, completely different situation can also occur: a non-terminal symbol may have only been defined in the target grammar as a facility for error tracking. In other words, the symbol had only rules containing the Bison 'error' keyword. Since these rules are skipped by the grammar input file parser, the resulting symbol effectively has no rules. A symbol in this situation should not be considered by a sentence generator, as it can never be involved in the derivations for a sintactically correct sentence. This leads to the need to completely wipe out all occurrences of the symbol from the rules of the grammar. The symbol is completely eliminated, so the user won't even find it in the formatted table printed by the \textbf{-p} option.
Note that a symbol which has only rules containing the \verb/error/ keyword, can be seen as an alias for the \verb/error/ keyword itself, so eliminating it from the rules of the grammar means literally removing the rules in which the symbol appears. This can cause the need for other symbols to be eliminated, so the check must reiterate several times.
If the starting symbol was found in the situation of having no valid rules, a fatal error is issued as no sentence generation can occur.

After this phase has been completed, control is passed to function \emph{normalize\_rules()} .
As an exception to the total compatibility with Bison's sintax, Forson extends it by accepting input grammar files in which there are multiple ``copies'' of the same rule. This means that two or more rules may be found which all have the same result and the same components, in the same order.
These rules are all appended to the non terminal symbol (the result). This facility is provided as a naive method for affecting the probability of being chosen by the ``random'' generation mode, which works on a stochastic grammar.
The \emph{normalize\_rules()} function reduces all the occurrences of the same rule in a single entry in the table, regulating the frequency value of the entry by a normalization relative to the total number of rules in the non-terminal. In other words, for example, if a non-terminal symbol has three ``real'' alternative rules, but the first one appears five times in the input grammar file, the second and third rules will each be chosen by the Grow algorithm five times less often than the first.
The normalization algorithm is run also when the ``coverage'' mode is selected, to assure that the final data structure will be presented to the Purdom algorithm in a canonical form, in which all the rules in a symbol are distinct.

The last check to be performed insures that the grammar does not contain irreducible symbols that lead to an infinite generation. That is, it must exist for every non-terminal symbol (all reachable at this point) a finite sequence of derivations which leads to a sentence of only terminal symbols.
If this condition doesn't hold, a fatal error is issued, indicating the culprit non-terminal which causes the inconsistency.
An error of this type could even be seen as a correct termination of the program. Mathematically, a situation of this type is infact allowed. The resulting grammar is simply said to define an empty language. Of course from Forson's point of view, it correctly means that no sentence generation is possible.


\section{Sentence Generation Algorithms}

\subsection{Grow}
Now that the internal data structures have been flawlessly constructed, Forson can proceed with the user selected algorithm for sentence generation.

We will now provide a formal description of the two algorithms used by the program.

The ``random'' mode relies on the Grow descent algorithm which operates on a stochastic grammar:

\begin{verbatim}
1: Start with the starting symbol (axiom) on the stack
2: REPEAT
        IF   the symbol on the stack is a terminal symbol
        THEN output it's associated text;
        ELSE (the symbol on the stack is a non-terminal)
             choose a random rule for the 
             symbol on the stack and push
             all components of the rule
             (from right to left) on the stack
   UNTIL stack is empty
\end{verbatim}

\noindent
A simple, dynamically growing stack implementation is used by the algorithm. The related functions can be found in \emph{stack.c}.

We must specify that the statement ``choose a random rule for the symbol on the stack'' is affected by the fact that the collection of distinct rules of every non-terminal symbol possess a probability density distribution. This is what we mean for stochastic grammar.
The statement ``output it's associated text'', from an implementation point of view, takes different meanings depending on what kind of terminal symbol is found. For a ``literal'', the associated text is simply it's name. For a ``lexical'', the associated text is a random string chosen in the set of lexical values provided in the lexicon input file; if none are found, it's name (as it appears in Bison's token definition) is used as a fallback alternative.

Every call to grow(AXIOM) produces a single sintactically valid sentence of the target grammar. Extra controls must be applied outside the Grow algorithm's implementation in order to reiterate the generation until the requested number of sentences has been generated (\textbf{-r} option). The \emph{grow()} function does not keep contextual state information between single calls. This implies, for example, that the same sentence can be generated more than once.

\subsection{Purdom}
The Purdom algorithm for coverage generation is formalized as follows:

\begin{verbatim}
1: Start with the starting symbol (axiom) on the stack;
2: REPEAT
        IF      symbol A on top of the stack is a terminal
        THEN    pop it and output associated text
        ELSE    pop A and put the components of
                rule CHOOSE(A) on the stack
        FI
   UNTIL stack is empty
\end{verbatim}

\noindent
The CHOOSE algorithm is defined as follows:

\begin{verbatim}
1: IF   an unused rule which has A as the result exists
   THEN choose it
   ELSE IF      some rules of A derive a
                symbol B not on the stack
                which has an unvisited rule
        THEN    choose one of them
        ELSE    choose the rule which is the
                first step of a derivation
                having as it's yield the
                shortest string derivable from A
        FI
   FI
\end{verbatim}

\noindent
The Purdom algorithm works perfectly well to obtain a set of sentences that use all rules of the grammar (and also all symbols). Of course every call to the function Purdom(AXIOM) produces only one sentence of the set. Extra controls must be applied outside the Purdom algorithm's implementation in order to reiterate the generation until there are no unused rules left in the grammar. For this reason the marking of the rules as ``visited'' should be applied statically.

Minimality of the set is not guaranteed by the Purdom algorithm. Anyway, the concept of minimality for a set of test sentences that cover all rules of a target grammar is not clearly defined. As a ``rule of thumb'', the Purdom algorithm tends to generate many, short sentences instead of less, longer sentences.


\section{Known Problems}
No facility for limiting the size of randomly generated sentence is implemented. Infinite generation may occur in ``random'' mode. For this reason caution should be used when applying the Grow algorithm to a grammar in which some non-terminal symbol has many recursive rules and few rules which lead to the terminal symbols. As we have seen, the stochastic grammar created by Forson assigns the same frequency of choice to all the rules of a non-terminal symbol. The only way to try to avoid this behavior is to place extra copies of non-recursive rules in the Bison grammar input file. This is not an elegant solution, and further work may be focused on creating a set of controls (perhaps checking the size of the stack), to limit or even specify a target size for generated sentences.

Error tracking for the Bison meta-grammar parser does not seem to work. More specifically the line number provided in the error message is completely erratic, even if the implementation complies with the guidelines provided in Bison's documentation.

\begin{thebibliography}{}

\bibitem{Crespi_et_al}
A. Celentano, S. Crespi Reghizzi, P. Della Vigna and C. Ghezzi, `Compiler Testing using a Sentence Generator', Report, Istituto di Elettrotecnica ed Elettronica, Politecnico di Milano, 1979.

\bibitem{Purdom}
P. Purdom, `A sentence generator for testing parsers', \emph{Bit}, 12, 366-375 (1972).

\end{thebibliography}

\end{document}
